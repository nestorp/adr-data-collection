{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import html\n",
    "import json\n",
    "import datetime\n",
    "import xml.etree.ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37458 urls extracted\n"
     ]
    }
   ],
   "source": [
    "site_url = \"\"\n",
    "url_filename = \"\"\n",
    "url_list = []\n",
    "for x in range(0,36):\n",
    "    filename = 'sitemaps/sitemap-discussion-'+str(x)+'.xml/sitemap-discussion-'+str(x)+'.xml'\n",
    "    e = xml.etree.ElementTree.parse(filename).getroot()\n",
    "    for el in e:\n",
    "        for url in el:\n",
    "            group = url.text.replace(site_url,'')\n",
    "            group = group[:group.find('/')]\n",
    "            if group.lower() == 'anxiety':# or group.lower() == 'depression':\n",
    "                url_list.append(url.text)\n",
    "                #print(url.text)\n",
    "                \n",
    "print(len(url_list), 'urls extracted')\n",
    "with open(url_filename, 'w', encoding='UTF-8') as txtstream:\n",
    "    for url in url_list:\n",
    "        txtstream.write(url+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(url_list), 'urls extracted')\n",
    "with open(url_filename, 'w', encoding='UTF-8') as txtstream:\n",
    "    for url in url_list:\n",
    "        txtstream.write(url+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_filename = \"\"\n",
    "now = datetime.datetime.now()\n",
    "print(\"Begin scraping\", str(now))\n",
    "\n",
    "urlnum = 0\n",
    "\n",
    "with open(scrape_filename, 'w', encoding='UTF-8') as txtstream:\n",
    "    for url in url_list:\n",
    "        \n",
    "        urlnum += 1\n",
    "        \n",
    "        try:\n",
    "            response = get(url)\n",
    "\n",
    "            html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            post_title = html_soup.find('div', class_ = 'group-breadcrumb__discussion-title')\n",
    "\n",
    "            p_cont = html_soup.find('div', class_ = 'group-discussion-container__post')\n",
    "\n",
    "            thread_pos = 0\n",
    "\n",
    "            ptitle = post_title.h1.get_text()\n",
    "            postedby = p_cont.find('span', class_ = 'newsfeed__posted-by').a.get_text()\n",
    "            postedtime = p_cont.find('time', class_ = 'newsfeed__item-time')['datetime']\n",
    "            pcontent = p_cont.find('div', class_ = 'posts__content').get_text().replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\t\",\" \")\n",
    "            totalcomments = p_cont.find('div', class_ = 'newsfeed__icon-count').get_text()\n",
    "\n",
    "            txtstream.write(url+'|'+ptitle+'|'+str(thread_pos)+'|'+str(totalcomments)+'|'+postedby +'|'+ postedtime +'|'+ pcontent+'\\n')\n",
    "            print(\"Scraping post \" + str(urlnum) + \" - \" + ptitle)\n",
    "\n",
    "            thread_pos+=1    \n",
    "            comment_containers = html_soup.find_all('div', class_ = 'comments__comment')\n",
    "            for c_cont in comment_containers:\n",
    "\n",
    "                postedby = c_cont.find('span', class_ = 'comments__name').a.get_text()\n",
    "                postedtime = c_cont.find('time', class_ = 'newsfeed__itemtime')['datetime']\n",
    "                pcontent = c_cont.find('div', class_ = 'comments__comment-text').get_text().replace(\"\\n\",\" \").replace(\"\\r\",\" \").replace(\"\\t\",\" \")\n",
    "\n",
    "                txtstream.write(url+'|'+ptitle+'|'+str(thread_pos)+'|'+str(totalcomments)+'|'+postedby +'|'+ postedtime +'|'+ pcontent+'\\n')\n",
    "\n",
    "                thread_pos+=1  \n",
    "        except Exception as e:\n",
    "             txtstream.write(url+'|'+str(e))\n",
    "                             \n",
    "        time.sleep(random.randint(1,4))\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"End scraping\", str(now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df_posts = pd.read_csv(scrape_filename, sep=\"|\", header=None, names = [\"url\", \"title\", \"thread_pos\", \"comment_num\", \"posted_by\", \n",
    "                                                            \"posted_time\", \"content\"], dtype={\"url\": str, \"title\": str, \"thread_pos\": int,\"comment_num\": int, \"posted_by\": str, \n",
    "                                                            \"posted_time\":str, \"content\":str}, parse_dates=[\"posted_time\"])\n",
    "\n",
    "df_posts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
